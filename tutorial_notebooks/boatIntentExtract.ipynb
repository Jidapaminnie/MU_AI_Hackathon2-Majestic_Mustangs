{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG application for \"A Guide for First-Time Parents\"\n",
    "\n",
    "## Install tika and parse PDF file\n",
    "\n",
    "- Install libraries\n",
    "- Download PDF from the website [The Asian Parent](https://th.theasianparent.com/%E0%B8%84%E0%B8%B9%E0%B9%88%E0%B8%A1%E0%B8%B7%E0%B8%AD%E0%B8%94%E0%B8%B9%E0%B9%81%E0%B8%A5%E0%B8%A5%E0%B8%B9%E0%B8%81)\n",
    "- Parse a PDF file using `tika`\n",
    "- Clean text (using a simple created function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "import tika\n",
    "tika.initVM()\n",
    "from tika import parser\n",
    "from unidecode import unidecode\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_book = parser.from_file(\"baby_0_3.pdf\")\n",
    "\n",
    "# n_pages = int(parsed_book[\"metadata\"][\"xmpTPg:NPages\"])\n",
    "# print(n_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"chief complaint\", \"making appointment\", \"medical experts\"]\n",
    "contents = []\n",
    "intents = []\n",
    "for file in files:\n",
    "    with open(f\"../data/{file}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        file_contents = f.readlines()\n",
    "        contents.extend()\n",
    "        intents.extend([file]*len(file_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    \"\"\"Clean parsed text from PDF for embedding\"\"\"\n",
    "    text = text.replace(\"\\uf70a\", \"่\")\n",
    "    text = text.replace(\"�ำ\", \"ำ\")\n",
    "    text = text.replace(\"�า\", \"ำ\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content = parsed_book[\"content\"]\n",
    "# content_processed = clean_text(content)\n",
    "# pages = content_processed.split(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages_strip = [\" \".join(page.split()) for page in pages]  # strip extra spaces from page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [clean_text(c) for  c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform RAG for each page in the book\n",
    "\n",
    "- As we skim through, each page already contains a single content\n",
    "- Chunk information to default `chunk_size` of 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_page_to_chunk(page_text, chunk_size: int = 2048):\n",
    "    chunks = [page_text[i:i + chunk_size] for i in range(0, len(page_text), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for text in pages_strip:\n",
    "    chunks.extend(convert_page_to_chunk(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"baby_0_3.json\", \"w\") as f:\n",
    "    json.dump(chunks, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting using RAG\n",
    "\n",
    "- Embed text chunks with and store using `faiss`\n",
    "- Embed query using the same embedding script\n",
    "- Find the closest text chunks\n",
    "- Add information and perform RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import TextGenerationModel, TextEmbeddingModel\n",
    "from llama_index.legacy.llms.vertex import Vertex\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAI\\miniconda3\\envs\\llama_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.vertexai.ChatVertexAI` was deprecated in langchain-community 0.0.12 and will be removed in 0.2.0. An updated version of the class exists in the langchain-google-vertexai package and should be used instead. To use it run `pip install -U langchain-google-vertexai` and import as `from langchain_google_vertexai import ChatVertexAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "emb_model_name = \"textembedding-gecko-multilingual@001\"\n",
    "gen_model_name  = \"text-bison\"\n",
    "service_account_path = \"credentials\\\\vertex-test-417403-ce72ad032af7.json\"\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "aiplatform.init(project=credentials.project_id, credentials=credentials)\n",
    "# emb_model = TextEmbeddingModel.from_pretrained(emb_model_name)\n",
    "# gen_model = TextGenerationModel.from_pretrained(gen_model_name)\n",
    "\n",
    "# vertex_ai = Vertex(model=\"text-bison\", project=credentials.project_id, location= \"asia-southeast1\", credentials=credentials, temperature=0.2)\n",
    "chat_vertex_ai = ChatVertexAI(model_name=\"chat-bison-32k\", project=credentials.project_id, location= \"asia-southeast1\", credentials=credentials, temperature=0.2, max_output_tokens= 8192) # max for bison 32k                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(texts: list[str], model, batch_size=128):\n",
    "    texts = [text.replace(\"\\n\", \" \") for text in texts]\n",
    "    embeddings = []\n",
    "    for idx in range(0, len(texts), batch_size):\n",
    "         embeddings.extend(model.get_embeddings(texts[idx:idx+batch_size]))\n",
    "    vectors = [emb.values for emb in embeddings]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt: str, temperature: float = 0.0, top_p: float = 0.95, top_k: int = 40, max_output_tokens: int = 2048):\n",
    "        parameters = {\n",
    "            'temperature': temperature,\n",
    "            'top_p': top_p,\n",
    "            'top_k': top_k,\n",
    "            'max_output_tokens': max_output_tokens\n",
    "        }\n",
    "        # return gen_model.predict(prompt, **parameters).text\n",
    "        return chat_vertex_ai.predict(prompt, **parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent_from_chat(text:str):\n",
    "    prompt = f\"Your task is to retrive intention of a given text. You should answer only 'medical experts' when the text is about finding medical expert, 'making appointment' when the text is about making appointment to the medical expert, 'chief complaint' when the text is about symptom. If the text is not related to what previous sentence mentioned, please answer 'unknown'. Text: `{text}`\"\n",
    "    intent = get_completion(prompt).strip()\n",
    "    if intent in [\"medical experts\", \"making appointment\", \"chief complaint\"]:\n",
    "        return intent\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medical experts'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_intent_from_chat(\"อยากหาหมอที่ตรวจหาสาเหตุของอาการหัวใจเต้นเร็วและเหนื่อยง่าย\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
